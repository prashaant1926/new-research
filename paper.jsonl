{"id":"paper_1702500000001","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","journal":"NAACL","year":"2019","doi":"10.18653/v1/N19-1423","url":"https://arxiv.org/abs/1810.04805","keyAssumptions":"Unidirectional language models are sufficient for pre-training; Fine-tuning requires task-specific architectures","citation":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (pp. 4171-4186).","notes":"Introduces bidirectional pre-training using masked language modeling. Shows that deep bidirectional representations significantly improve downstream task performance.","addedDate":"2024-01-15T10:05:00Z"}